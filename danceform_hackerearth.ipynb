{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "danceform_hackerearth.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "TXOFZA24_NDz",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "10d077b2-e57d-4362-b9fc-e07fcc70abc0"
      },
      "source": [
        "import numpy as np # MATRIX OPERATIONS\n",
        "import pandas as pd # EFFICIENT DATA STRUCTURES\n",
        "import matplotlib.pyplot as plt # GRAPHING AND VISUALIZATIONS\n",
        "import math # MATHEMATICAL OPERATIONS\n",
        "import cv2 # IMAGE PROCESSING - OPENCV\n",
        "from glob import glob # FILE OPERATIONS\n",
        "import itertools\n",
        "\n",
        "# KERAS AND SKLEARN MODULES\n",
        "from keras.utils import np_utils\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "from keras.layers import Dropout\n",
        "from keras.layers import Flatten\n",
        "from keras.layers.convolutional import Conv2D\n",
        "from keras.layers.convolutional import MaxPooling2D\n",
        "from keras.layers import BatchNormalization\n",
        "from keras.callbacks import ModelCheckpoint,ReduceLROnPlateau,CSVLogger\n",
        "\n",
        "from sklearn import preprocessing\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from tqdm import tqdm\n",
        "from keras.preprocessing import image\n",
        "import os\n",
        "import random\n"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wkLVoV3R_W2m",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "import zipfile"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H68JxMUZ_Y1K",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "zip_ref = zipfile.ZipFile(\"/content/drive/My Drive/dance.zip\", 'r')\n",
        "zip_ref.extractall(\"/dance_ml\")\n",
        "zip_ref.close()"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xldlemVlFmLV",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 195
        },
        "outputId": "655e7f42-5441-4b3d-9b68-a13f0bf01948"
      },
      "source": [
        "train = pd.read_csv('/dance_ml/dataset/train.csv')\n",
        "train.head()"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Image</th>\n",
              "      <th>target</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>96.jpg</td>\n",
              "      <td>manipuri</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>163.jpg</td>\n",
              "      <td>bharatanatyam</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>450.jpg</td>\n",
              "      <td>odissi</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>219.jpg</td>\n",
              "      <td>kathakali</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>455.jpg</td>\n",
              "      <td>odissi</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "     Image         target\n",
              "0   96.jpg       manipuri\n",
              "1  163.jpg  bharatanatyam\n",
              "2  450.jpg         odissi\n",
              "3  219.jpg      kathakali\n",
              "4  455.jpg         odissi"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "podvCtm4_gXA",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 168
        },
        "outputId": "8e672f64-f704-4903-fcf2-bc1d5bc0d792"
      },
      "source": [
        "train['target'].value_counts()"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "mohiniyattam     50\n",
              "odissi           49\n",
              "kathakali        47\n",
              "bharatanatyam    47\n",
              "kuchipudi        46\n",
              "sattriya         45\n",
              "kathak           44\n",
              "manipuri         36\n",
              "Name: target, dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vvyorn1mAPIu",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 195
        },
        "outputId": "987786ac-acc7-430e-a8fa-0197ea704cc4"
      },
      "source": [
        "test = pd.read_csv('/dance_ml/dataset/test.csv')\n",
        "test.head()"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Image</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>508.jpg</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>246.jpg</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>473.jpg</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>485.jpg</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>128.jpg</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "     Image\n",
              "0  508.jpg\n",
              "1  246.jpg\n",
              "2  473.jpg\n",
              "3  485.jpg\n",
              "4  128.jpg"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wVLgNMPJAzzu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "Class_map={'manipuri':0,'kathak':1,'sattriya':2,'kuchipudi':3,'kathakali':4,'bharatanatyam':5,'odissi':6,'mohiniyattam':7}\n",
        "inverse_map={0:'manipuri',1:'kathak',2:'sattriya',3:'kuchipudi',4:'kathakali',5:'bharatanatyam',6:'odissi',7:'mohiniyattam'}\n",
        "\n",
        "\n"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uxR7pVPABRVi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train['target']=train['target'].map(Class_map)"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gz2gCrBhB4_n",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "da1c0dd6-8321-4be1-f587-46217bd12ab4"
      },
      "source": [
        "train_img=[]\n",
        "train_label=[]\n",
        "j=0\n",
        "path='/dance_ml/dataset/train'\n",
        "for i in tqdm(train['Image']):\n",
        "    final_path=os.path.join(path,i)\n",
        "    img=cv2.imread(final_path)\n",
        "    img=cv2.resize(img,(224,224))\n",
        "    img=img.astype('float32')\n",
        "    train_img.append(img)\n",
        "    train_label.append(train['target'][j])\n",
        "    j=j+1"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 364/364 [00:02<00:00, 170.10it/s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ffQphRTDC91y",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "569523f7-9097-4164-83ac-15e1d29cccdd"
      },
      "source": [
        "test_img=[]\n",
        "path='/dance_ml/dataset/test'\n",
        "for i in tqdm(test['Image']):\n",
        "    final_path=os.path.join(path,i)\n",
        "    img=cv2.imread(final_path)\n",
        "    img=cv2.resize(img,(224,224))\n",
        "    img=img.astype('float32')\n",
        "    test_img.append(img)"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 156/156 [00:00<00:00, 163.78it/s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NIDNyLn4EXUR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "datagenerator = ImageDataGenerator(\n",
        "        featurewise_center=False,  \n",
        "        samplewise_center=False,  \n",
        "        featurewise_std_normalization=False,  \n",
        "        samplewise_std_normalization=False,  \n",
        "        rotation_range = 10,  \n",
        "        zoom_range = 0.10,  \n",
        "        width_shift_range=0.10,  \n",
        "        height_shift_range=0.10,  \n",
        "        horizontal_flip=True,  \n",
        "        vertical_flip=False) \n",
        "\n",
        "\n",
        "datagenerator.fit(train_img)"
      ],
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4hHVA4lbKq7G",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 67
        },
        "outputId": "6a7bb4ff-c64d-45ef-c49c-7b1077ce02e2"
      },
      "source": [
        "train_img=np.array(train_img)\n",
        "test_img=np.array(test_img)\n",
        "train_label=np.array(train_label)\n",
        "print(train_img.shape)\n",
        "print(test_img.shape)\n",
        "print(train_label.shape)"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(364, 224, 224, 3)\n",
            "(156, 224, 224, 3)\n",
            "(364,)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Eg1x0jN9JIcW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "from keras.applications import VGG16\n",
        "from keras.applications.vgg16 import preprocess_input\n",
        "from keras.layers import *\n",
        "from keras.models import Sequential\n",
        "vgg16 = VGG16(weights='imagenet', include_top=False, input_shape = (224, 224, 3),pooling='avg')\n",
        "vgg16.trainable = False\n",
        "\n",
        "model = Sequential([\n",
        "  vgg16, \n",
        "  Dense(1024, activation='relu'),\n",
        "  Dropout(0.5),\n",
        "  \n",
        "  \n",
        "  \n",
        " \n",
        "  Dense(8, activation='softmax'),\n",
        "])"
      ],
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "azn99HLFRnes",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "bb42b43e-4eef-40ee-c291-3bb1b09c5c11"
      },
      "source": [
        "from keras.optimizers import Adam,SGD,Adagrad,Adadelta,RMSprop\n",
        "from keras.utils import to_categorical\n",
        "vgg16.trainable=False\n",
        "\n",
        "reduce_learning_rate = ReduceLROnPlateau(monitor='loss',\n",
        "                                         factor=0.2,\n",
        "                                         patience=2,\n",
        "                                         cooldown=2,\n",
        "                                         min_lr=0.00001,\n",
        "                                         verbose=1)\n",
        "\n",
        "callbacks = [reduce_learning_rate]\n",
        "    \n",
        "\n",
        "\n",
        "model.compile( optimizer='adam',loss='categorical_crossentropy',metrics=['accuracy'])\n",
        "model.fit_generator(datagenerator.flow(train_img, to_categorical(train_label,8), batch_size=64),\n",
        "                    epochs=35,steps_per_epoch = 10, callbacks=callbacks)"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/35\n",
            "10/10 [==============================] - 9s 858ms/step - loss: 7.0774 - accuracy: 0.2700\n",
            "Epoch 2/35\n",
            "10/10 [==============================] - 8s 840ms/step - loss: 2.8361 - accuracy: 0.5290\n",
            "Epoch 3/35\n",
            "10/10 [==============================] - 8s 820ms/step - loss: 1.5265 - accuracy: 0.6850\n",
            "Epoch 4/35\n",
            "10/10 [==============================] - 8s 821ms/step - loss: 0.9356 - accuracy: 0.7767\n",
            "Epoch 5/35\n",
            "10/10 [==============================] - 8s 836ms/step - loss: 0.5618 - accuracy: 0.8258\n",
            "Epoch 6/35\n",
            "10/10 [==============================] - 8s 817ms/step - loss: 0.4895 - accuracy: 0.8383\n",
            "Epoch 7/35\n",
            "10/10 [==============================] - 8s 843ms/step - loss: 0.3536 - accuracy: 0.8823\n",
            "Epoch 8/35\n",
            "10/10 [==============================] - 8s 815ms/step - loss: 0.3121 - accuracy: 0.9033\n",
            "Epoch 9/35\n",
            "10/10 [==============================] - 8s 808ms/step - loss: 0.2213 - accuracy: 0.9267\n",
            "Epoch 10/35\n",
            "10/10 [==============================] - 8s 823ms/step - loss: 0.1565 - accuracy: 0.9500\n",
            "Epoch 11/35\n",
            "10/10 [==============================] - 8s 805ms/step - loss: 0.1755 - accuracy: 0.9483\n",
            "Epoch 12/35\n",
            "10/10 [==============================] - 8s 839ms/step - loss: 0.1240 - accuracy: 0.9597\n",
            "Epoch 13/35\n",
            "10/10 [==============================] - 8s 831ms/step - loss: 0.0876 - accuracy: 0.9800\n",
            "Epoch 14/35\n",
            "10/10 [==============================] - 8s 846ms/step - loss: 0.0865 - accuracy: 0.9774\n",
            "Epoch 15/35\n",
            "10/10 [==============================] - 8s 813ms/step - loss: 0.0954 - accuracy: 0.9633\n",
            "Epoch 16/35\n",
            "10/10 [==============================] - 8s 815ms/step - loss: 0.0811 - accuracy: 0.9733\n",
            "Epoch 17/35\n",
            "10/10 [==============================] - 8s 846ms/step - loss: 0.0674 - accuracy: 0.9806\n",
            "Epoch 18/35\n",
            "10/10 [==============================] - 8s 819ms/step - loss: 0.0922 - accuracy: 0.9683\n",
            "Epoch 19/35\n",
            "10/10 [==============================] - 8s 833ms/step - loss: 0.0841 - accuracy: 0.9717\n",
            "\n",
            "Epoch 00019: ReduceLROnPlateau reducing learning rate to 0.00020000000949949026.\n",
            "Epoch 20/35\n",
            "10/10 [==============================] - 8s 823ms/step - loss: 0.0626 - accuracy: 0.9767\n",
            "Epoch 21/35\n",
            "10/10 [==============================] - 8s 848ms/step - loss: 0.0640 - accuracy: 0.9790\n",
            "Epoch 22/35\n",
            "10/10 [==============================] - 8s 847ms/step - loss: 0.0579 - accuracy: 0.9887\n",
            "Epoch 23/35\n",
            "10/10 [==============================] - 8s 799ms/step - loss: 0.0471 - accuracy: 0.9862\n",
            "Epoch 24/35\n",
            "10/10 [==============================] - 8s 838ms/step - loss: 0.0356 - accuracy: 0.9919\n",
            "Epoch 25/35\n",
            "10/10 [==============================] - 8s 846ms/step - loss: 0.0428 - accuracy: 0.9935\n",
            "Epoch 26/35\n",
            "10/10 [==============================] - 8s 818ms/step - loss: 0.0413 - accuracy: 0.9883\n",
            "\n",
            "Epoch 00026: ReduceLROnPlateau reducing learning rate to 4.0000001899898055e-05.\n",
            "Epoch 27/35\n",
            "10/10 [==============================] - 8s 812ms/step - loss: 0.0329 - accuracy: 0.9933\n",
            "Epoch 28/35\n",
            "10/10 [==============================] - 8s 842ms/step - loss: 0.0328 - accuracy: 0.9935\n",
            "Epoch 29/35\n",
            "10/10 [==============================] - 8s 821ms/step - loss: 0.0354 - accuracy: 0.9967\n",
            "Epoch 30/35\n",
            "10/10 [==============================] - 8s 815ms/step - loss: 0.0270 - accuracy: 0.9933\n",
            "Epoch 31/35\n",
            "10/10 [==============================] - 8s 809ms/step - loss: 0.0451 - accuracy: 0.9833\n",
            "Epoch 32/35\n",
            "10/10 [==============================] - 8s 826ms/step - loss: 0.0458 - accuracy: 0.9900\n",
            "\n",
            "Epoch 00032: ReduceLROnPlateau reducing learning rate to 1e-05.\n",
            "Epoch 33/35\n",
            "10/10 [==============================] - 8s 831ms/step - loss: 0.0244 - accuracy: 0.9984\n",
            "Epoch 34/35\n",
            "10/10 [==============================] - 8s 838ms/step - loss: 0.0363 - accuracy: 0.9935\n",
            "Epoch 35/35\n",
            "10/10 [==============================] - 8s 783ms/step - loss: 0.0445 - accuracy: 0.9879\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.callbacks.History at 0x7fdbf4da6eb8>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ipjn5tknJ9k7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "labels = model.predict(test_img)\n",
        "label = [np.argmax(i) for i in labels]\n",
        "class_label = [inverse_map[x] for x in label]\n",
        "submission = pd.DataFrame({ 'Image': test.Image, 'target': class_label })\n",
        "submission.to_csv('result5.csv', index=False)"
      ],
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "48C7g5Z5xgPq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}